{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76624639-8ca7-4724-a06b-3685e97ec55c",
   "metadata": {},
   "source": [
    "# Notebook for statistical analysis of model training and performance eval\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f196a8-fdb8-47d8-8b56-5cdbae1c5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from agents import MinimaxAgent, RandomAgent, AvoidNextLossAgent, ModelAgent, NeuroevolutionAgent\n",
    "from environment import ConnectFourEnv\n",
    "from evaluation_func import EvaluationFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69013a6-5aa8-4ecf-9678-da0bbf13d138",
   "metadata": {},
   "source": [
    "## Training performance evaluation\n",
    "The goal is to visualize the training performance of each model in a graph regarding loss and average return since those metrics were collected.\n",
    "### Preparing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddce01a-3259-4489-aa69-ce317d33ed34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>MODEL</th>\n",
       "      <th>DATE</th>\n",
       "      <th>EPOCH</th>\n",
       "      <th>EPSILON</th>\n",
       "      <th>AVERAGE LOSS</th>\n",
       "      <th>AVERAGE RETURN</th>\n",
       "      <th>PATH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [NAME, MODEL, DATE, EPOCH, EPSILON, AVERAGE LOSS, AVERAGE RETURN, PATH]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the initial dictionary by grid and loading the train data\n",
    "dfs = {}\n",
    "train_data = pd.read_csv('./training_data.csv',index_col=None)\n",
    "\n",
    "#splitting the training data by model training data\n",
    "dfs['6by7'] = train_data[train_data['PATH'] == './weights/d6by7.h5']\n",
    "dfs['8by9'] = train_data[train_data['PATH'] == './weights/d8by9.h5']\n",
    "dfs['10by11'] = train_data[train_data['PATH'] == './weights/d10by11.h5']\n",
    "dfs['12by13'] = train_data[train_data['PATH'] == './weights/d12by13.h5']\n",
    "\n",
    "#making the epoch the index for each dataframe\n",
    "dfs['6by7'].set_index('EPOCH')\n",
    "dfs['8by9'].set_index('EPOCH')\n",
    "dfs['10by11'].set_index('EPOCH')\n",
    "dfs['12by13'].set_index('EPOCH')\n",
    "\n",
    "dfs['6by7'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0829b-7769-4194-977a-260e7d56b657",
   "metadata": {},
   "source": [
    "### Visualization functions\n",
    "We write a basic function for visualizing training process in matplotlib and save the figure at a desired path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4235b3-1e43-4350-a076-ad7496213a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_progress(df, name : str, col_val : str, path : str, show=False):\n",
    "    '''\n",
    "    Function to show and also save a graph containing the reward over all episodes.\n",
    "    :param epoch_returns (list): A list of acerage returns for n epochs (defined as 1000 training steps).\n",
    "    '''\n",
    "\n",
    "    x = np.arange(0, len())\n",
    "    y = df[col_val].numpy()\n",
    "\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x,y)\n",
    "\n",
    "    ax.set_title(f'Training progress for model {name}')\n",
    "    ax.set_ylabel(f' {col_val} per epoch')\n",
    "    ax.set_xlabel('Epochs')\n",
    "\n",
    "    fig.savefig(path)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb78a94-971a-4059-84c1-9a36029fcd88",
   "metadata": {},
   "source": [
    "We then run the function for each model we trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7340f8-5f14-46d7-8d59-267b27d60a92",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() takes exactly one argument (0 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# visualize_progress(dfs['6by7'], 'DQN-6x7', 'AVERAGE LOSS', './figures/6by7avgloss.png')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# visualize_progress(dfs['6by7'], 'DQN-6x7', 'AVERAGE RETURN', './figures/6by7avgret.png')\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mvisualize_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m8by9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDQN-8x9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAVERAGE LOSS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./figures/8by9avgloss.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m visualize_progress(dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8by9\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDQN-8x9\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAVERAGE RETURN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./figures/8by9avgret.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m visualize_progress(dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10by11\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDQN-10x11\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAVERAGE LOSS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./figures/10by11avgloss.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [3], line 7\u001b[0m, in \u001b[0;36mvisualize_progress\u001b[1;34m(df, name, col_val, path, show)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisualize_progress\u001b[39m(df, name : \u001b[38;5;28mstr\u001b[39m, col_val : \u001b[38;5;28mstr\u001b[39m, path : \u001b[38;5;28mstr\u001b[39m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Function to show and also save a graph containing the reward over all episodes.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    :param epoch_returns (list): A list of acerage returns for n epochs (defined as 1000 training steps).\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[col_val]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdark_background\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: len() takes exactly one argument (0 given)"
     ]
    }
   ],
   "source": [
    "visualize_progress(dfs['6by7'], 'DQN-6x7', 'AVERAGE LOSS', './figures/6by7avgloss.png')\n",
    "visualize_progress(dfs['6by7'], 'DQN-6x7', 'AVERAGE RETURN', './figures/6by7avgret.png')\n",
    "\n",
    "visualize_progress(dfs['8by9'], 'DQN-8x9', 'AVERAGE LOSS', './figures/8by9avgloss.png')\n",
    "visualize_progress(dfs['8by9'], 'DQN-8x9', 'AVERAGE RETURN', './figures/8by9avgret.png')\n",
    "\n",
    "visualize_progress(dfs['10by11'], 'DQN-10x11', 'AVERAGE LOSS', './figures/10by11avgloss.png')\n",
    "visualize_progress(dfs['10by11'], 'DQN-10x11', 'AVERAGE RETURN', './figures/10by11avgret.png')\n",
    "\n",
    "visualize_progress(dfs['12by13'], 'DQN-12x13', 'AVERAGE LOSS', './figures/12by13avgloss.png')\n",
    "visualize_progress(dfs['12by13'], 'DQN-12x13', 'AVERAGE RETURN', './figures/12by13avgret.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e13ad-376f-4dd9-8b81-c7567b892036",
   "metadata": {},
   "source": [
    "## Agent performance evaluation\n",
    "Our goal is to evaluate the performance of each model against each other per grid size of the Environment.\n",
    "If there is a significant advantage of one model over another based on statistics it can be called better.\n",
    "\n",
    "### Agent initialization\n",
    "We firstly initialize every agent we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b5c5cf-511c-4f03-a5e7-2dcc8dd40d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimax = MinimaxAgent()\n",
    "random = RandomAgent()\n",
    "lossavoid = AvoidNextLossAgent()\n",
    "\n",
    "dqn = {}\n",
    "dqn['6by7'] = ModelAgent('d6by7')\n",
    "dqn['8by9'] = ModelAgent('d8by9')\n",
    "dqn['10by11'] = ModelAgent('d10by11')\n",
    "dqn['12by13'] = ModelAgent('d12by13')\n",
    "\n",
    "neuroevo = NeuroevolutionAgent('Evolved_ANN.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f502a3-e978-4fc2-ac25-a3dcc1e347bb",
   "metadata": {},
   "source": [
    "### Evaluation function initialization\n",
    "We also initialize the evaluation function with parameters which we want to use for minimax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a312c4-7de5-447d-8439-e8cfa27bac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = None #TODO\n",
    "eval_func = EvaluationFunction(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8c294-3b18-4d22-8790-988090dfc036",
   "metadata": {},
   "source": [
    "### Writing the match functions and the pandas dataframe for storing the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9158ee-f6a0-47f3-ad90-d19c3568b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_agents(agent_a, agent_b, gridsize : tuple, playouts=200):\n",
    "    '''\n",
    "    Function for playout between two different agents on a Connect-Four grid of fixed size.\n",
    "    Agents A and B will play against each other and statistics will be collected and stored in their object instance.\n",
    "    Note that on uneven (even by index since it starts from 0) playouts A will be first to move and on even playouts B will have the first move.\n",
    "    \n",
    "    :param agent_a (): An agent who can play on the specified gridsize.\n",
    "    :param agent_b (): An agent who can play on the specified gridsize.\n",
    "    :param gridsize (tuple): A valid gridsize for the environment and agents, accepts (6,7), (8,9), (10,11), (12,13).\n",
    "    :param playouts (int): The number of games to be played.\n",
    "    '''\n",
    "    \n",
    "    env = ConnectFourEnv(gridsize)\n",
    "    \n",
    "    time_a = []\n",
    "    time_b = []\n",
    "    for i in tqdm(range(playouts),desc='Matches processed:'):\n",
    "        env.reset() #reset environment once\n",
    "        \n",
    "        while not env.terminal:\n",
    "            if (env.turn and (i % 2) == 0) or (not env.turn and (i % 2) == 1):\n",
    "                starttime = datetime.now()\n",
    "                action = agent_a.select_move(env, eval_func=eval_func)\n",
    "                time_a += [(datetime.now()-starttime).microseconds*(10**-6)]\n",
    "                env.step(action)\n",
    "                \n",
    "            elif (not env.turn and (i % 2) == 0) or (env.turn and (i % 2) == 1):\n",
    "                starttime = datetime.now()\n",
    "                action = agent_b.select_move(env, eval_func=eval_func)\n",
    "                time_b += [(datetime.now()-starttime).microseconds*(10**-6)]\n",
    "                env.step(action)\n",
    "        \n",
    "        #after each played match statistics of wins and time are gathered\n",
    "        if (i % 2) == 0:\n",
    "            if env.winner:\n",
    "                agent_a.wins += 1\n",
    "                agent_b.losses += 1\n",
    "                \n",
    "            else:\n",
    "                agent_a.losses += 1\n",
    "                agent_b.wins += 1\n",
    "        else:\n",
    "            if env.winner:\n",
    "                agent_b.wins += 1\n",
    "                agent_a.losses += 1\n",
    "                \n",
    "            else:\n",
    "                agent_b.losses += 1\n",
    "                agent_a.wins += 1\n",
    "    \n",
    "    #after every match was played the time statistics are added\n",
    "    agent_a.average_speed = float(np.mean(time_a))\n",
    "    agent_b.average_speed = float(np.mean(time_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f065c5f4-d47c-4753-98aa-2c35a8e19e1c",
   "metadata": {},
   "source": [
    "### Now for each agent we can start to collect data and store it in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1145b6-4f0e-48c1-907e-67f8b446eac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGENT A</th>\n",
       "      <th>AGENT B</th>\n",
       "      <th>GRID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [AGENT A, AGENT B, GRID, A, B]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['AGENT A','AGENT B', 'GRID', 'A', 'B']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa19fe-a927-4411-b0f6-d9c94cb6c655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Putting RandomAgent vs AvoidNextLossAgent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matches processed:: 100%|███████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 131.10it/s]\n",
      "C:\\Users\\marlo\\AppData\\Local\\Temp\\ipykernel_12192\\4049234098.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(appendix)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent performance (16, 184, 200) with avg speed 4.312729498164015e-05 secs/move.\n",
      "Agent performance (184, 16, 200) with avg speed 0.000777435975609756 secs/move.\n",
      "\n",
      "\n",
      "Putting RandomAgent vs MinimaxAgent:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matches processed::   4%|██▏                                                           | 7/200 [00:24<11:09,  3.47s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_for_idx = {0 : 'RandomAgent', 1 : 'AvoidNextLossAgent', 2 : 'MinimaxAgent', 3 : 'ModelAgent', 4 : 'NeuroevolutionAgent'}\n",
    "for gridsize in [(6, 7), (8, 9), (10, 11), (12, 13)]:\n",
    "    for i, agent_a in enumerate([random, lossavoid, minimax, dqn, neuroevo]):\n",
    "        for j, agent_b in enumerate([random, lossavoid, minimax, dqn, neuroevo]):\n",
    "            if i != j and j > i and (not (((i or j) == 4) and gridsize != (6, 7))):\n",
    "                print(f'Putting {name_for_idx[i]} vs {name_for_idx[j]}:')\n",
    "                match_agents(agent_a, agent_b, gridsize)\n",
    "                appendix = pd.DataFrame([[name_for_idx[i],name_for_idx[j],str(gridsize),agent_a.wins,agent_b.wins]],columns=columns)\n",
    "                print(agent_a)\n",
    "                print(agent_b)\n",
    "                print('\\n')\n",
    "                agent_a.reset()\n",
    "                agent_b.reset()\n",
    "                df = df.append(appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "408a0951-b809-43a2-b046-e81f3b6949ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matches processed::   0%|                                                                      | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model does not support input shape of passed environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmatch_agents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdqn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m8by9\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [8], line 29\u001b[0m, in \u001b[0;36mmatch_agents\u001b[1;34m(agent_a, agent_b, gridsize, playouts)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m env\u001b[38;5;241m.\u001b[39mturn \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (env\u001b[38;5;241m.\u001b[39mturn \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     28\u001b[0m     starttime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 29\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     time_b \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [(datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m-\u001b[39mstarttime)\u001b[38;5;241m.\u001b[39mmicroseconds\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m)]\n\u001b[0;32m     31\u001b[0m     env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\DRL-SuSe-2022---Group-2\\final\\agents.py:168\u001b[0m, in \u001b[0;36mModelAgent.select_move\u001b[1;34m(self, env, eval_func)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_move\u001b[39m(\u001b[38;5;28mself\u001b[39m, env, eval_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    ADD\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(policy)\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(action)\n",
      "File \u001b[1;32m~\\DRL-SuSe-2022---Group-2\\final\\agents.py:157\u001b[0m, in \u001b[0;36mModelAgent.policy\u001b[1;34m(self, env)\u001b[0m\n\u001b[0;32m    154\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(obs\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;66;03m#may not work change after testing\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size:\n\u001b[1;32m--> 157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel does not support input shape of passed environment.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    159\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(obs)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy\n",
      "\u001b[1;31mValueError\u001b[0m: Model does not support input shape of passed environment."
     ]
    }
   ],
   "source": [
    "match_agents(random, dqn['8by9'], (8,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3204b4a-a266-493e-9902-b47f2f3176f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./performance_data.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5e3325-a9b6-4c58-827e-fb1d1af0cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD ANALYSIS FUNCTIONS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c98f26-6ac9-439c-bad4-f01441158649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD VISUALIZATION FUNCTIONS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c7089-b847-465d-aefc-10ada2df9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALL VISUALIZATION AND ANALYSIS FUNCTIONS HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
